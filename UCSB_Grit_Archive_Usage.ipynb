{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planet GRIT Archive Use Report\n",
    "\n",
    "Based on Planet's Customer Usage Analysis Template! \n",
    "<strong>How it Works:</strong>\n",
    "\n",
    "+ This code uses the reports API to collect summary level daily usage. Each request will capture a 90-day snapshot and auto compile the results into a single data frame. From here the newly formed data frame will be manipulated into a series of visuals that will download to your working direcotry. Each of these visuals will provide you insight into your customer's usage and may even be useful to share with customers directly. \n",
    "\n",
    "\n",
    "+ Once you've input the necessary information, you can execute each code block. Keep in mind that the processing time may vary depending on the number of usage reports requested from teh Reports API. On average, each 90-day report takes approximately 1-2 minutes to generate.\n",
    "\n",
    "With this template, you'll gain deep insights into your customer base, allowing you to make data-driven decisions and optimize your services effectively.\n",
    "\n",
    "*For questions regarding this code, please message <strong>Austin Stone</strong> on slack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# Use an organization admin API key. Must be sitting within the parent organization if you are collecting information about suborgs\n",
    "# os.environ['PL_API_KEY']='' \n",
    "PLANET_API_KEY = os.getenv('PL_API_KEY')\n",
    "\n",
    "# Input the contract and start and end dates\n",
    "contract_start = \"2023-02-01\"\n",
    "contract_end = \"2026-01-31\"\n",
    "\n",
    "# Create a directory to save the CSV files\n",
    "csv_dir = 'UCSB_Usage'\n",
    "os.makedirs(csv_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Report_Type': 'detailed',\n",
       "  'Cadence': 'daily',\n",
       "  'SubOrgs': {'Include': 'all'},\n",
       "  'Toi': {'start_date': '2023-02-01', 'end_date': '2023-05-02'}},\n",
       " {'Report_Type': 'detailed',\n",
       "  'Cadence': 'daily',\n",
       "  'SubOrgs': {'Include': 'all'},\n",
       "  'Toi': {'start_date': '2023-05-02', 'end_date': '2023-07-31'}},\n",
       " {'Report_Type': 'detailed',\n",
       "  'Cadence': 'daily',\n",
       "  'SubOrgs': {'Include': 'all'},\n",
       "  'Toi': {'start_date': '2023-07-31', 'end_date': '2023-10-29'}},\n",
       " {'Report_Type': 'detailed',\n",
       "  'Cadence': 'daily',\n",
       "  'SubOrgs': {'Include': 'all'},\n",
       "  'Toi': {'start_date': '2023-10-29', 'end_date': '2024-01-27'}},\n",
       " {'Report_Type': 'detailed',\n",
       "  'Cadence': 'daily',\n",
       "  'SubOrgs': {'Include': 'all'},\n",
       "  'Toi': {'start_date': '2024-01-27', 'end_date': '2024-04-26'}},\n",
       " {'Report_Type': 'detailed',\n",
       "  'Cadence': 'daily',\n",
       "  'SubOrgs': {'Include': 'all'},\n",
       "  'Toi': {'start_date': '2024-04-26', 'end_date': '2024-07-25'}},\n",
       " {'Report_Type': 'detailed',\n",
       "  'Cadence': 'daily',\n",
       "  'SubOrgs': {'Include': 'all'},\n",
       "  'Toi': {'start_date': '2024-07-25', 'end_date': '2024-10-23'}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the initial payload\n",
    "initial_payload = {\n",
    "    \"Report_Type\": \"detailed\",\n",
    "    \"Cadence\": \"daily\",\n",
    "    \"SubOrgs\": {\n",
    "        \"Include\": \"all\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the start date and end date\n",
    "start_date = datetime.strptime(contract_start, \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(contract_end, \"%Y-%m-%d\")\n",
    "\n",
    "# Define the interval (90 days)\n",
    "interval = timedelta(days=90)\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Create a list to store payloads\n",
    "payloads = []\n",
    "\n",
    "# Create a list to store references to the payload dictionaries\n",
    "payload_variables = []\n",
    "\n",
    "# Generate payloads for each 90-day interval\n",
    "while start_date <= current_date:\n",
    "    # Calculate the end date for this interval\n",
    "    interval_end_date = min(start_date + interval, end_date)\n",
    "\n",
    "    # Create a new payload dictionary for this interval\n",
    "    payload = initial_payload.copy()\n",
    "\n",
    "    # Update the payload with the current interval's dates\n",
    "    payload[\"Toi\"] = {\n",
    "        \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"end_date\": interval_end_date.strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "\n",
    "    # Append the payload to the list of payloads\n",
    "    payloads.append(payload)\n",
    "\n",
    "    # Add a reference to the payload dictionary to the list of payload variable names\n",
    "    payload_variables.append(payload)\n",
    "\n",
    "    # Move to the next interval\n",
    "    start_date += interval\n",
    "\n",
    "payload_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1 successful\n",
      "Request 1 is in progress (Attempt 1)\n",
      "Request 1 is completed\n",
      "CSV file 1 for request 1 saved as UCSB_Usage\\result_1_1.csv\n",
      "Request 2 successful\n",
      "Request 2 is in progress (Attempt 1)\n",
      "Request 2 is completed\n",
      "CSV file 1 for request 2 saved as UCSB_Usage\\result_2_1.csv\n",
      "Request 3 successful\n",
      "Request 3 is in progress (Attempt 1)\n",
      "Request 3 is completed\n",
      "CSV file 1 for request 3 saved as UCSB_Usage\\result_3_1.csv\n",
      "Request 4 successful\n",
      "Request 4 is in progress (Attempt 1)\n",
      "Request 4 is completed\n",
      "CSV file 1 for request 4 saved as UCSB_Usage\\result_4_1.csv\n",
      "Request 5 successful\n",
      "Request 5 is in progress (Attempt 1)\n",
      "Request 5 is completed\n",
      "CSV file 1 for request 5 saved as UCSB_Usage\\result_5_1.csv\n",
      "Request 6 successful\n",
      "Request 6 is in progress (Attempt 1)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "url = \"https://api.planet.com/reports/v0/downloads/suborgs\"\n",
    "\n",
    "# Define the maximum number of retries and the delay between retries (in seconds)\n",
    "max_retries = 10\n",
    "retry_delay = 60  # 1 minute\n",
    "\n",
    "# Function to check the status of response_v2\n",
    "def check_status():\n",
    "    response_v2 = requests.request(\"GET\", url_v2, auth=HTTPBasicAuth(PLANET_API_KEY, ''))\n",
    "    if response_v2.status_code == 200:\n",
    "        return response_v2.json().get('status')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Loop through the list of payloads and make requests for each one\n",
    "for idx, payload_var in enumerate(payload_variables):\n",
    "    # Make the POST request for the current payload\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        auth=HTTPBasicAuth(PLANET_API_KEY, ''),\n",
    "        json=payload_var\n",
    "    )\n",
    "    \n",
    "    # Check the response status code and handle it as needed\n",
    "    if response.status_code == 201:\n",
    "        print(f\"Request {idx + 1} successful\")\n",
    "        \n",
    "        # You can process the response data here if needed\n",
    "        geojson = response.json()\n",
    "        response_id = geojson['id']\n",
    "        url_v2 = \"https://api.planet.com/reports/v0/downloads/suborgs\" + \"/\" + response_id\n",
    "        \n",
    "        # Implement the retry mechanism to check the status\n",
    "        for attempt in range(max_retries):\n",
    "            response_v2 = requests.request(\"GET\", url_v2, auth=HTTPBasicAuth(PLANET_API_KEY, ''))\n",
    "            status = check_status()\n",
    "\n",
    "            if status == \"COMPLETED\":\n",
    "                print(f\"Request {idx + 1} is completed\")\n",
    "                break  # Exit the retry loop\n",
    "\n",
    "            elif status == \"IN_PROGRESS\":\n",
    "                print(f\"Request {idx + 1} is in progress (Attempt {attempt + 1})\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to retrieve status for request {idx + 1} (Attempt {attempt + 1})\")\n",
    "\n",
    "            # Wait for the specified delay before the next retry\n",
    "            time.sleep(retry_delay)\n",
    "        \n",
    "        # Check if the status is \"COMPLETED\" before downloading the CSV\n",
    "        if status == \"COMPLETED\":\n",
    "            geojson_v2 = response_v2.json()\n",
    "            # Get the download links for the CSV files (assuming they're in 'contents')\n",
    "            download_links = geojson_v2.get('contents', [])\n",
    "            \n",
    "            if download_links:\n",
    "                # Iterate through each download link and download the CSV files\n",
    "                for file_idx, download_link in enumerate(download_links):\n",
    "                    csv_download_url = download_link\n",
    "                    \n",
    "                    # Make a request to download the CSV file\n",
    "                    response_v3 = requests.get(csv_download_url, auth=HTTPBasicAuth(PLANET_API_KEY, ''))\n",
    "                    \n",
    "                    if response_v3.status_code == 200:\n",
    "                        # Get the CSV file content\n",
    "                        csv_content = response_v3.content\n",
    "                        \n",
    "                        # Create a file path to save the CSV file\n",
    "                        csv_filename = os.path.join(csv_dir, f'result_{idx + 1}_{file_idx + 1}.csv')\n",
    "                        \n",
    "                        # Save the CSV file locally\n",
    "                        with open(csv_filename, 'wb') as csv_file:\n",
    "                            csv_file.write(csv_content)\n",
    "                        \n",
    "                        print(f\"CSV file {file_idx + 1} for request {idx + 1} saved as {csv_filename}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to download CSV file {file_idx + 1} for request {idx + 1}\")\n",
    "            else:\n",
    "                print(f\"No CSV files found for request {idx + 1}\")\n",
    "        else:\n",
    "            print(f\"Request {idx + 1} did not complete successfully\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Request {idx + 1} failed with status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PLANET_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store individual dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each CSV file and read it into a dataframe\n",
    "for csv_file in csv_files:\n",
    "    csv_path = os.path.join(csv_dir, csv_file)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Now, combined_df contains all the data from your CSV files in a single dataframe\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = os.path.join(csv_dir, 'all_use.csv')\n",
    "print(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(output_filepath, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Convert the 'Date' column to a datetime type\n",
    "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "\n",
    "# Calculate the cumulative quota used by date\n",
    "cumulative_quota_by_date = combined_df.groupby('Date')['Quota_Used__sqkm_'].sum().cumsum()\n",
    "\n",
    "# Calculate the overall quota used since the first date\n",
    "overall_quota_used = cumulative_quota_by_date.iloc[-1]  # Get the last value in the cumulative sum\n",
    "\n",
    "# Display the overall quota used since the first date in a larger font with a professional background\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.text(0.5, 0.5, f\"Overall Quota Used Since Contract Start:\\n{overall_quota_used:,.2f} sqkm\",\n",
    "         fontsize=16, bbox=dict(facecolor='white', alpha=0.7))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Group the data by user and sum their quota usage\n",
    "user_quota_usage = combined_df.groupby('User_Email')['Quota_Used__sqkm_'].sum().reset_index()\n",
    "\n",
    "# Sort the data by quota used in descending order within each user group\n",
    "quota_by_user = combined_df.groupby('User_Email')\n",
    "combined_df_sorted = quota_by_user['Quota_Used__sqkm_'].sum().reset_index().sort_values(by=['Quota_Used__sqkm_'], ascending=False)\n",
    "\n",
    "# Create a bar plot of quota used by user with custom colors and sorted data\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.barplot(x='Quota_Used__sqkm_', y='User_Email', data=combined_df_sorted, palette=\"viridis\")\n",
    "\n",
    "# Add data labels to the bars and adjust label position\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    plt.text(width + 0.1, p.get_y() + p.get_height() / 2, f'{width:,.2f}', ha=\"left\")\n",
    "\n",
    "plt.title('Quota Used by User Email')\n",
    "plt.xlabel('Quota Used)')\n",
    "plt.ylabel('User Email')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter('{x:,.0f}'))\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.5)  # Add horizontal grid lines\n",
    "plt.tight_layout()\n",
    "output_filepath = os.path.join(csv_dir,'user_quota_usage.png')\n",
    "plt.savefig(output_filepath, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Create a line plot of cumulative quota used by date\n",
    "plt.figure(figsize=(12, 10))\n",
    "cumulative_quota_by_date.plot()\n",
    "plt.title('Cumulative Quota Used by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Quota Used (sqkm)')\n",
    "plt.gca().yaxis.set_major_formatter(mtick.StrMethodFormatter('{x:,.0f}'))\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
    "output_filepath = os.path.join(csv_dir,'cumulative_quota_usage.png')\n",
    "plt.savefig(output_filepath, dpi=300)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Group the data by organization and sum their quota usage\n",
    "org_quota_usage = combined_df.groupby('Organization_Name')['Quota_Used__sqkm_'].sum().reset_index()\n",
    "\n",
    "# Sort the data by quota usage in descending order within each organization\n",
    "org_quota_usage_sorted = org_quota_usage.sort_values(by='Quota_Used__sqkm_', ascending=False)\n",
    "\n",
    "# Create a bar plot to visualize quota usage by organization\n",
    "plt.figure(figsize=(14, 10))\n",
    "ax = sns.barplot(x='Quota_Used__sqkm_', y='Organization_Name', data=org_quota_usage_sorted, palette='viridis')\n",
    "\n",
    "# Add data labels to the bars and adjust label position\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    plt.text(width + 0.1, p.get_y() + p.get_height() / 2, f'{width:,.2f}', ha=\"left\")\n",
    "\n",
    "plt.title('Quota Usage by Organization')\n",
    "plt.xlabel('Quota Used (sqkm)')\n",
    "plt.ylabel('Organization Name')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter('{x:,.0f}'))  # Format x-axis with commas and 2 decimals\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.5)  # Add horizontal grid lines\n",
    "plt.tight_layout()\n",
    "output_filepath = os.path.join(csv_dir, 'org_quota_usage.png')\n",
    "plt.savefig(output_filepath, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the distribution of total quota used by \"item_type\" as percentages\n",
    "item_type_distribution = combined_df.groupby('Item_Type')['Quota_Used__sqkm_'].sum() / overall_quota_used * 100\n",
    "\n",
    "# Create a bar plot to visualize the distribution of total quota used by \"item_type\"\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.barplot(x=item_type_distribution.values, y=item_type_distribution.index, palette=\"Blues_d\")\n",
    "plt.title('Distribution of Total Quota Used by Item Type (%)')\n",
    "plt.xlabel('Percentage of Total Quota Used (%)')\n",
    "plt.ylabel('Item Type')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(decimals=2))  # Format x-axis as percentages with 2 decimals\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.5)  # Add horizontal grid lines\n",
    "plt.tight_layout()\n",
    "\n",
    "output_filepath = os.path.join(csv_dir, 'item_type_distribution.png')\n",
    "plt.savefig(output_filepath, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
